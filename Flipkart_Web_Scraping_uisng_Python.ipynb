{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flipkart Web Scraping uisng Python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1rupUKYiCaSbuw1kuVQ4vcwZgFMkJsA6j",
      "authorship_tag": "ABX9TyOpqykhMKgKFjYLUor7dHCo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pranay318/Webscraping/blob/main/Flipkart_Web_Scraping_uisng_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJar83iT36np"
      },
      "source": [
        "#import libraries\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "import time\n",
        "\n",
        "import smtplib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Connect to Website and pull in data\n",
        "link='https://www.flipkart.com/search?q=tv&as=on&as-show=on&otracker=AS_Query_TrendingAutoSuggest_8_0_na_na_na&otracker1=AS_Query_TrendingAutoSuggest_8_0_na_na_na&as-pos=8&as-type=TRENDING&suggestionId=tv&requestId=9c9fa553-b7e5-454b-a65b-bbb7a9c74a29'\n",
        "\n",
        "page = requests.get(link)\n",
        "\n",
        "page.content\n",
        "\n",
        "soup = bs(page.content, 'html.parser')#it gives us the visual representation of data\n",
        "print(soup.prettify())"
      ],
      "metadata": {
        "id": "XCbFYuhIF9hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = soup.find('div',class_=\"_4rR01T\")\n",
        "print(name.text)\n",
        "\n",
        "rating=soup.find('div', class_=\"_3LWZlK\")\n",
        "print(rating.text)\n",
        "\n",
        "specification=soup.find('div',class_=\"fMghEO\")\n",
        "print(specification.text)\n",
        "\n",
        "for each in specification:\n",
        "  spec=each.find_all('li',class_=\"rgWa7D\")\n",
        "  print(spec[0].text)\n",
        "  print(spec[1].text)\n",
        "  print(spec[2].text)\n",
        "  print(spec[4].text)\n",
        "  print(spec[5].text)\n",
        "  print(spec[7].text)\n",
        "\n",
        "price=soup.find('div',class_=\"_3I9_wc _27UcVY\")\n",
        "print(price.text)"
      ],
      "metadata": {
        "id": "TC7HXcbaMRFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "products=[] #list to store the name of the product\n",
        "prices=[] #list to store price of the product\n",
        "ratings=[] #list to store rating of the product\n",
        "apps=[] #list to store supported apps\n",
        "os=[] #list to store operating system\n",
        "hd=[] #list to store resolution\n",
        "sound=[] #list to store sound output"
      ],
      "metadata": {
        "id": "RCZ3RR7zuIKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in soup.findAll('div',class_=\"_3pLy-c row\"):\n",
        "  names=data.find('div',attrs={'class':'_4rR01T'})\n",
        "  price=data.find('div',attrs={'class':'_3I9_wc _27UcVY'})\n",
        "  rating=data.find('div',attrs={'class':'_3LWZlK'})\n",
        "  specification=data.find('div',attrs={'class':'fMghEO'})\n",
        "  for each in specification:\n",
        "    col=each.find_all('li', attrs={'class':'rgWa7D'})\n",
        "    app = col[0].text\n",
        "    os_ = col[1].text\n",
        "    hd_ = col[2].text\n",
        "    sound_ = col[3].text\n",
        "    products.append(names.text) #Add product name to list\n",
        "    prices.append(price.text) #Add price to list\n",
        "    apps.append(app)#Add suported apps specifications to list\n",
        "    os.append(os_)#Add operating system specificatios to list\n",
        "    hd.append(hd_)#Add resolution specifications to list\n",
        "    sound.append(sound_)#Add sound specifications to list\n",
        "    ratings.append(rating.text)#Add rating specifications to list"
      ],
      "metadata": {
        "id": "cQxFs7vD-hxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing the length of list\n",
        "print(len(products))\n",
        "print(len(ratings))\n",
        "print(len(prices))\n",
        "print(len(apps))\n",
        "print(len(sound))\n",
        "print(len(os))\n",
        "print(len(hd))"
      ],
      "metadata": {
        "id": "MbabkYJoI2he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a timestamp for your output to track when data was collected\n",
        "import datetime\n",
        "today = datetime.date.today()\n",
        "print(today)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HreqKsIhNZBR",
        "outputId": "2a97ef79-6398-4033-9a17-92c412bbd6a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({'Product Name':products,'Supported_apps':apps,'sound_system':sound,'OS':os,'Price':prices,'Rating':ratings,'Date':today})\n",
        "#saving the dataframe \n",
        "df.to_csv(r'/content/drive/MyDrive/Daily Python/Web Scraping/Dataset/FlipkartWebScraper.csv', index=False)\n",
        "df.head(25)"
      ],
      "metadata": {
        "id": "3XIc53UOJI0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine all the above code into one function\n",
        "def check_price():\n",
        "  link='https://www.flipkart.com/search?q=tv&as=on&as-show=on&otracker=AS_Query_TrendingAutoSuggest_8_0_na_na_na&otracker1=AS_Query_TrendingAutoSuggest_8_0_na_na_na&as-pos=8&as-type=TRENDING&suggestionId=tv&requestId=9c9fa553-b7e5-454b-a65b-bbb7a9c74a29'\n",
        "  \n",
        "  page = requests.get(link)\n",
        "  \n",
        "  page.content\n",
        "  \n",
        "  soup = bs(page.content, 'html.parser')#it gives us the visual representation of data\n",
        "  \n",
        "  print(soup.prettify())\n",
        "\n",
        "  name = soup.find('div',class_=\"_4rR01T\")\n",
        "  print(name.text)\n",
        "  \n",
        "  rating=soup.find('div', class_=\"_3LWZlK\")\n",
        "  print(rating.text)\n",
        "  \n",
        "  specification=soup.find('div',class_=\"fMghEO\")\n",
        "  print(specification.text)\n",
        "  \n",
        "  for each in specification:\n",
        "    spec=each.find_all('li',class_=\"rgWa7D\")\n",
        "    print(spec[0].text)\n",
        "    print(spec[1].text)\n",
        "    print(spec[2].text)\n",
        "    print(spec[4].text)\n",
        "    print(spec[5].text)\n",
        "    print(spec[7].text)\n",
        "    \n",
        "    price=soup.find('div',class_=\"_3I9_wc _27UcVY\")\n",
        "    print(price.text)\n",
        "    \n",
        "    for data in soup.findAll('div',class_=\"_3pLy-c row\"):\n",
        "      names=data.find('div',attrs={'class':'_4rR01T'})\n",
        "      price=data.find('div',attrs={'class':'_3I9_wc _27UcVY'})\n",
        "      rating=data.find('div',attrs={'class':'_3LWZlK'})\n",
        "      specification=data.find('div',attrs={'class':'fMghEO'})\n",
        "      for each in specification:\n",
        "        col=each.find_all('li', attrs={'class':'rgWa7D'})\n",
        "        app = col[0].text\n",
        "        os_ = col[1].text\n",
        "        hd_ = col[2].text\n",
        "        sound_ = col[3].text\n",
        "        products.append(names.text) #Add product name to list\n",
        "        prices.append(price.text) #Add price to list\n",
        "        apps.append(app)#Add suported apps specifications to list\n",
        "        os.append(os_)#Add operating system specificatios to list\n",
        "        hd.append(hd_)#Add resolution specifications to list\n",
        "        sound.append(sound_)#Add sound specifications to list\n",
        "        ratings.append(rating.text)#Add rating specifications to list\n",
        "\n",
        "      #Create a timestamp for your output to track when data was collected\n",
        "      import datetime\n",
        "      today = datetime.date.today()\n",
        "      print(today)\n",
        "\n",
        "      import pandas as pd\n",
        "      df = pd.DataFrame({'Product Name':products,'Supported_apps':apps,'sound_system':sound,'OS':os,'Price':prices,'Rating':ratings,'Date':today})\n",
        "      #saving the dataframe \n",
        "      df.to_csv(r'/content/drive/MyDrive/Daily Python/Web Scraping/Dataset/FlipkartWebScraper.csv', index=False)\n",
        "      df.head(25)"
      ],
      "metadata": {
        "id": "DHo4Kt5XFpxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(r'/content/drive/MyDrive/Daily Python/Web Scraping/Dataset/FlipkartWebScraper.csv')\n",
        "\n",
        "df.head(25)"
      ],
      "metadata": {
        "id": "gyPKJWTrFpuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If you want to try sending yourself an email when a price hits below certain level \n",
        "\n",
        "def send_mail():\n",
        "  server = smtplib.SMTP_SSL('smtp.gmail.com',465)\n",
        "  server.ehlo()\n",
        "  #server.starttls()\n",
        "  server.login('gopranayshinde@gmail.com','xxxxxxxxxxxxxxx')\n",
        "\n",
        "  subject = \"The TV you want to but is below â‚¹38,000! Now is your chance to buy!\"\n",
        "  body = \"Pranay, This is the momen we have been waiting for.Now is your chance to pick up the Television of your dreams. Don't mess it up! Link here:https://www.flipkart.com/search?q=tv&as=on&as-show=on&otracker=AS_Query_OrganicAutoSuggest_3_2_na_na_na&otracker1=AS_Query_OrganicAutoSuggest_3_2_na_na_na&as-pos=3&as-type=RECENT&suggestionId=tv&requestId=f57a3712-d714-44fe-b270-8efe5581b9b3&as-searchtext=tv&sort=recency_desc\"\n",
        "\n",
        "  msg = f\"Subject: {subject}\\n\\n{body}\"\n",
        "\n",
        "  server.sendmail(\n",
        "      'gopranayshinde@gmail.com',\n",
        "      msg\n",
        "  )"
      ],
      "metadata": {
        "id": "cadztGKRFpi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "wYLJVwUNTH7y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}